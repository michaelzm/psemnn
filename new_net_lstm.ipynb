{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import Input,Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import zipfile\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import backend as keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from dataset_joiner import DatasetWorker, VocabularyWorker\n",
    "from performance import PerformanceViewer, TrainingEval\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234567890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "# specify the type of information which shall be extracted\n",
    "#extraction_of = 'contexts'\n",
    "extraction_of = 'sentiments'\n",
    "#extraction_of = 'aspects'\n",
    "\n",
    "#sentiment, aspect oder modifier -> diese drei braucht man\n",
    "#extraktion von polarität nicht gefragt\n",
    "\n",
    "\n",
    "# specify filenames in the next line\n",
    "if extraction_of in ['contexts']:\n",
    "    filename = r'data_laptop_ctxt.json'\n",
    "elif extraction_of in ['sentiments','aspects']:\n",
    "    filename = r'data_laptop_absa.json'\n",
    "\n",
    "## in this example, we use the glove word embeddings as input for the neural network\n",
    "## download glove.42B.300d.txt from http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
    "filename_embedding_zip = r'glove.42B.300d.zip' # folder of downloaded glove zip file\n",
    "## specify folder where to store the glove embeddings\n",
    "filepath_embedding = filename_embedding_zip.replace('.zip','')\n",
    "## unzip and save glove to a folder manually or with the next lines\n",
    "if not os.path.exists(filepath_embedding):\n",
    "    with zipfile.ZipFile(filename_embedding_zip,\"r\") as zip_ref:\n",
    "        zip_ref.extractall(filepath_embedding)\n",
    "os.listdir(filepath_embedding)[0]\n",
    "filename_embedding = filepath_embedding + '/' + os.listdir(filepath_embedding)[0]\n",
    "\n",
    "\n",
    "with open(filename,'r', encoding='utf8') as infile:\n",
    "    example_data = json.load(infile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenize dataset: 3101it [00:00, 74836.66it/s]\n",
      "split dataset tokens: 3101it [00:00, 1329775.76it/s]\n",
      "split dataset labels: 3101it [00:00, 138050.19it/s]\n",
      "update train tokens: 100%|██████████| 2480/2480 [00:00<00:00, 706553.04it/s]\n",
      "update test tokens: 100%|██████████| 621/621 [00:00<00:00, 96626.46it/s]\n",
      "update train labels: 100%|██████████| 2480/2480 [00:00<00:00, 22539.71it/s]\n",
      "update test labels: 100%|██████████| 621/621 [00:00<00:00, 760263.51it/s]\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 100\n",
    "ds = DatasetWorker(example_data)\n",
    "ds.applyPreprocessing()\n",
    "ds.splitDatasetTokens()\n",
    "ds.setExtractionOf(\"sentiments\")\n",
    "ds.splitDatasetLabels(\"union\")\n",
    "ds.buildDatasetSequence(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build vocabulary: 100%|██████████| 3101/3101 [00:00<00:00, 42006.30it/s]\n",
      "build embedding vectors: 100%|██████████| 4562/4562 [00:00<00:00, 343884.38it/s]\n",
      "build labelclasses: 100%|██████████| 2480/2480 [00:00<00:00, 564521.54it/s]\n"
     ]
    }
   ],
   "source": [
    "#build vocab and add embedding\n",
    "vw = VocabularyWorker()\n",
    "vw.buildVocabulary(ds.dataset)\n",
    "vw.buildEmbedding(ds.train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting tokens & labels to ids : 100%|██████████| 2480/2480 [00:00<00:00, 27481.11it/s]\n",
      "Converting tokens & labels to ids : 100%|██████████| 621/621 [00:00<00:00, 39034.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert data to Input format for neural network\n",
    "x_train, y_train = vw.convert_tokens_labels_list_to_ids_list(ds.train_tokens, ds.train_labels, max_seq_length)\n",
    "x_test, y_test = vw.convert_tokens_labels_list_to_ids_list(ds.test_tokens, ds.test_labels, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer', 'works', 'great', '.']\n",
      "['bough', 'this', 'for', 'the', 'google', 'goodies', 'offers', 'for', 'the', 'holidays', 'but', 'when', 'i', 'go', 'to', 'redeem', 'the', 'offers', 'from', 'the', 'google', 'website', ',', 'it', 'says', 'that', 'this', \"doesn't\", 'have', 'the', 'right', '\"', 'code', '\"', 'to', 'get', 'the', 'drive', 'storage', '.', 'sending', 'back', '.']\n",
      "[521 557 123   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "[4181   22   11    2  682 3236  551   11    2 1749   36   64   13  140\n",
      "    5 3563    2  551   28    2  682  314    1   18  295   15   22 4550\n",
      "   29    2  155    9  489    9    5   77    2  582 1028    3 1926  126\n",
      "    3    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "for a in [ds.train_tokens, x_train]:\n",
    "    for i, e in enumerate(a):\n",
    "        if i < 2:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make classes cateogrical\n",
    "y_train = to_categorical(y_train, num_classes = vw.n_tags)\n",
    "y_test = to_categorical(y_test, num_classes = vw.n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 300)          1452000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 200)          320800    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 100, 3)            603       \n",
      "=================================================================\n",
      "Total params: 1,773,403\n",
      "Trainable params: 1,773,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# the following two layers should not be changed.\n",
    "input_layer = Input(shape=(max_seq_length,))\n",
    "embedding_layer = Embedding(vw.vocab_size, 300, weights=[vw.embedding_vectors], input_length=max_seq_length)(input_layer)\n",
    "\n",
    "lstm_layer = Dropout(0.1)(embedding_layer)\n",
    "lstm_layer = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(lstm_layer)\n",
    "# here, attention models have to be implemented in this model\n",
    "#nur bestimmten wörtern aufmerksamkeit geben\n",
    "# ...\n",
    "\n",
    "# this last layer can/should be modified\n",
    "output_layer = TimeDistributed(Dense(vw.n_tags, activation=\"softmax\"))(lstm_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='rmsprop', metrics=[\"categorical_accuracy\", \"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = PerformanceViewer()\n",
    "evaluate_callback = TrainingEval(model, x_test, y_test, vw, ds, performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "78/78 [==============================] - 16s 203ms/step - loss: 0.1074 - categorical_accuracy: 0.9607 - accuracy: 0.9607 - val_loss: 0.0760 - val_categorical_accuracy: 0.9771 - val_accuracy: 0.9771\n",
      "Epoch 2/10\n",
      "78/78 [==============================] - 16s 201ms/step - loss: 0.0666 - categorical_accuracy: 0.9775 - accuracy: 0.9775 - val_loss: 0.0657 - val_categorical_accuracy: 0.9786 - val_accuracy: 0.9786\n",
      "Epoch 3/10\n",
      "78/78 [==============================] - 15s 187ms/step - loss: 0.0596 - categorical_accuracy: 0.9794 - accuracy: 0.9794 - val_loss: 0.0635 - val_categorical_accuracy: 0.9792 - val_accuracy: 0.9792\n",
      "Epoch 4/10\n",
      "78/78 [==============================] - 15s 193ms/step - loss: 0.0553 - categorical_accuracy: 0.9804 - accuracy: 0.9804 - val_loss: 0.0612 - val_categorical_accuracy: 0.9799 - val_accuracy: 0.9799\n",
      "Epoch 5/10\n",
      "78/78 [==============================] - 14s 180ms/step - loss: 0.0508 - categorical_accuracy: 0.9815 - accuracy: 0.9815 - val_loss: 0.0623 - val_categorical_accuracy: 0.9788 - val_accuracy: 0.9788\n",
      "Epoch 6/10\n",
      "78/78 [==============================] - 15s 190ms/step - loss: 0.0472 - categorical_accuracy: 0.9829 - accuracy: 0.9829 - val_loss: 0.0640 - val_categorical_accuracy: 0.9780 - val_accuracy: 0.9780\n",
      "Epoch 7/10\n",
      "78/78 [==============================] - 14s 182ms/step - loss: 0.0438 - categorical_accuracy: 0.9838 - accuracy: 0.9838 - val_loss: 0.0655 - val_categorical_accuracy: 0.9776 - val_accuracy: 0.9776\n",
      "Epoch 8/10\n",
      "15/78 [====>.........................] - ETA: 11s - loss: 0.0421 - categorical_accuracy: 0.9846 - accuracy: 0.9846"
     ]
    }
   ],
   "source": [
    "# fit model on train data\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=32,\n",
    "    #validation_split = 0.2,\n",
    "    verbose = 1,\n",
    "    callbacks = [evaluate_callback],\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.evalModelTrainDataClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.basicEval(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.classicEval(model, ds,vw,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
